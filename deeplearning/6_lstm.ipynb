{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified data/text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('data/text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "class BatchGenerator:\n",
    "    \n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293273 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.93\n",
      "================================================================================\n",
      "tnwdynuu tjrbv wg jai   pfzitbuageharsfivunaftb ztjebm y k dlexccsudttywvemam if\n",
      "n atynailpelyhrgrltiiwrnndginfzsw r ypebiqjseai ndqvthathuie hapchczkajgebbclati\n",
      "okqskm erhtdi y o cvoq  pkdhuujimia ymicfaoeyenhzrcoodg ya  ipi  xaedmvoxouamykk\n",
      "luooh a poskpxiew  nawo kg   bewuamcfguajscsgtga dghppdhsurl uumiyciilh crjyebfw\n",
      "vp  arh gen ehfc aoseyvh bgc  vbdzbtrroofwoni k pcxzfnnxsyds v rt ahewv jxcmjihc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.05\n",
      "Average loss at step 100: 2.595368 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.02\n",
      "Validation set perplexity: 10.39\n",
      "Average loss at step 200: 2.249264 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 8.60\n",
      "Average loss at step 300: 2.093618 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 400: 1.994112 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 500: 1.933963 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 600: 1.902536 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 700: 1.855474 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 800: 1.814363 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 900: 1.828842 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1000: 1.822730 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "amblion who fix a joegal prodrice mill greetarre ardea ar weat with curre aple s\n",
      "zer obfition and dargy nigh to in nerf whats the genece arove wo fowna the seven\n",
      "ver were wali m nieation recopted a the ceutie zesormens meromer of the ing the \n",
      "ber elen will user the colued by a three secen occun new tamfed the creased plet\n",
      "d henrecol the syer coppremeny mecile other seners pertuar with the esident late\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.773523 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1200: 1.753408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1300: 1.733338 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1400: 1.747332 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1500: 1.736656 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1600: 1.747838 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1700: 1.711687 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1800: 1.674407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1900: 1.646689 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2000: 1.695534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "winsion concititue gen be found in dedicard moke are umbeate pr kam and midding \n",
      "e the sincivens didef brove bot wownions wound dig zoss wht focon shump dys cmul\n",
      "ovies in butsi the dyss schowory puider a mamely lome tainote optoling a for a h\n",
      "quanism for with to and cressoage was with south yrigk of cheist whe computent p\n",
      "ge alings of he and infints fime camades bottle the new elandr which soutpior fr\n",
      "================================================================================\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2100: 1.684204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2200: 1.682274 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2300: 1.639453 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2400: 1.662360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2500: 1.678335 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2600: 1.654965 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2700: 1.658904 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2800: 1.654646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2900: 1.654473 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3000: 1.653981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "y populty of cassing carro kumpater of wheney and in the blysides is pet earcha \n",
      "bs hoodd ussemical heiling left by stouze and hoxern as american bablier is phin\n",
      "vileation expentity far to gusensivers caltade furs nrich actional whicl in the \n",
      "fiems and fauning the large to dis tradeniss his takenors her write had the rick\n",
      "mixal nus after rafila college to cars way as no p his the systables so sinces o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3100: 1.628169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3200: 1.644759 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3300: 1.638338 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3400: 1.668931 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3500: 1.655268 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3600: 1.670314 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3700: 1.649275 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3800: 1.643878 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3900: 1.635763 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4000: 1.656845 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "ber on two zero zero jeperded sinite the megensely compence the in margen the pa\n",
      "ospear or and to four alonging of the chiifos gay monas hadka the belaze it on f\n",
      "fles b ber wairal is displacen prize of u sekede adre cogelde in referement the \n",
      "ed the screezenves of retracity arianto mid the free the convers of the election\n",
      " compopen dibrety creect profess shud canazeldired main by dith stay m war casid\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4100: 1.634368 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4200: 1.637403 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4300: 1.615793 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4400: 1.612217 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4500: 1.612512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4600: 1.617032 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4700: 1.625538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4800: 1.631515 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4900: 1.633080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5000: 1.610244 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "================================================================================\n",
      "nisian history two zero zero tychory of the fall that impers asia amuiquri is ca\n",
      "johy he was hunkh me populard respine mid in higllatic son onedsins that and the\n",
      "al and yatle upre wepperdition day prevication in twf subs wides the remigu clla\n",
      "ling couds for a repropp the usin country such of the griinter of but marlion go\n",
      "wint as wincer the two zero pretiteoted every accorder hand mass abts game in se\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5100: 1.605421 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5200: 1.593662 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5300: 1.578275 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5400: 1.577736 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5500: 1.567453 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5600: 1.581324 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5700: 1.570392 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5800: 1.586123 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5900: 1.576619 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000: 1.548829 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "nicle was the confect to state one nine eight eight different to weast the panda\n",
      "th counted by his apppired vanuese reference west man ak di fidmests number well\n",
      "practide r dounda autic new recorded symm the countrics with thoughs id his fina\n",
      "onolite fan onl onint tatuments characters under the as kendure incorp navial ar\n",
      "fil langinged it in the times in emplanks him many more so theodovy people of a \n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6100: 1.564345 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6200: 1.535514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6300: 1.544529 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6400: 1.540784 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6500: 1.564327 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6600: 1.599242 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6700: 1.580826 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6800: 1.607778 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6900: 1.583079 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 7000: 1.575605 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "area towo this three four one four abolig zero bewinery becomedion supritage the\n",
      "herlus now trials and line lete puckaking that which used was perfoteless thimsl\n",
      "letic grice photoryse datics in one shows fe particlemal spating on alphar goy p\n",
      "ram common mar bock knows as edy an incluamellit the god un as the one nine th c\n",
      "ve in the waron the reasine currented misiap wills functions prodople ground nee\n",
      "================================================================================\n",
      "Validation set perplexity: 4.29\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def run_graph(graph):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized')\n",
    "        mean_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batches = train_batches.next()\n",
    "            feed_dict = dict()\n",
    "            for i in range(num_unrollings + 1):\n",
    "                feed_dict[train_data[i]] = batches[i]\n",
    "            _, l, predictions, lr = session.run(\n",
    "                [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "            mean_loss += l\n",
    "            if step % summary_frequency == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / summary_frequency\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "                mean_loss = 0\n",
    "                labels = np.concatenate(list(batches)[1:])\n",
    "                print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "                if step % (summary_frequency * 10) == 0:\n",
    "                    # Generate some samples.\n",
    "                    print('=' * 80)\n",
    "                    for _ in range(5):\n",
    "                        feed = sample(random_distribution())\n",
    "                        sentence = characters(feed)[0]\n",
    "                        reset_sample_state.run()\n",
    "                        for _ in range(79):\n",
    "                            prediction = sample_prediction.eval({sample_input: feed})\n",
    "                            feed = sample(prediction)\n",
    "                            sentence += characters(feed)[0]\n",
    "                        print(sentence)\n",
    "                    print('=' * 80)\n",
    "                # Measure validation set perplexity.\n",
    "                reset_sample_state.run()\n",
    "                valid_logprob = 0\n",
    "                for _ in range(valid_size):\n",
    "                    b = valid_batches.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n",
    "run_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how argument `i` to `lstm_cell()` is multiplied with all matrices with the `x` postfix,\n",
    "and the `o` is multiplied with matrices with the `m` postfix,\n",
    "and the biases are added.\n",
    "\n",
    "```\n",
    "input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "return output_gate * tf.tanh(state), state\n",
    "```\n",
    "\n",
    "As the problem states, these multiplications can be performed in a single multiply.\n",
    "\n",
    "Steps:\n",
    "- concatenate the input and output weights along with the biases; [tf.concat](https://www.tensorflow.org/api_docs/python/tf/concat)\n",
    "- perform the matrix multiplications on these concatenated matrices\n",
    "- split the result up into four matrices; [tf.split](https://www.tensorflow.org/api_docs/python/tf/split)\n",
    "- use the activation function on each split matrix\n",
    "- compute the state\n",
    "- compute the result\n",
    "\n",
    "## Trouble with `NaN`s\n",
    "\n",
    "Experienced `NaN` values in perplexity after a few steps.\n",
    "\n",
    "Tried the following:\n",
    "\n",
    "- stddev to `2/(in-neurons+output-neurons)`\n",
    "    - delayed NaN values to a later step\n",
    "- decrease learning rate to 5.0 (steps_before_decay=5000, decay=0.25)\n",
    "    - most often did not give `NaN`s during training\n",
    "- xavier initialization, with original learning rate of 10.0\n",
    "    - `NaN`s appeared later in the training\n",
    "- xavier initialization, with learning rate of 5.0 (`NaN`s appeared later in the training)\n",
    "- stddev=`2/(in-neurons+output-neurons)`, mean=-stddev, learning rate 5.0 \n",
    "    - often no `NaN`s\n",
    "    - sometimes `NaN` appear right before the learning rate is decayed\n",
    "    - tried decreases the steps before decaying, didn't help..\n",
    "- stddev=`2/(in-neurons+output-neurons)`, mean=-stddev, learning rate 10.0 \n",
    "    - *first run:* NaN in the beginning\n",
    "    - *second run:* NaN a bit later\n",
    " \n",
    "**xaviar initialization**:\n",
    "```\n",
    "init_range = math.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "mean=-init_range, stddev=init_range\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "#lstm_in_stddev = np.sqrt(6.0 / (vocabulary_size + num_nodes))\n",
    "#lstm_out_stddev = np.sqrt(6.0 / (num_nodes+num_nodes))\n",
    "#output_layer_stddev = np.sqrt(6.0 / (num_nodes+vocabulary_size))\n",
    "lstm_in_stddev = 2.0 / (vocabulary_size + num_nodes)\n",
    "lstm_out_stddev = 2.0 / (num_nodes+num_nodes)\n",
    "output_layer_stddev = 2.0 / (num_nodes+vocabulary_size)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -lstm_in_stddev, stddev=lstm_in_stddev))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -lstm_out_stddev, stddev=lstm_out_stddev))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -lstm_in_stddev, stddev=lstm_in_stddev))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -lstm_out_stddev, stddev=lstm_out_stddev))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -lstm_in_stddev, stddev=lstm_in_stddev))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -lstm_out_stddev, stddev=lstm_out_stddev))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -lstm_in_stddev, stddev=lstm_in_stddev))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -lstm_out_stddev, stddev=lstm_out_stddev))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -output_layer_stddev, stddev=output_layer_stddev))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Concatenate matrices for LSTM cell into 4x input, output and biases\n",
    "    lstm_in   = tf.concat([ix, fx, cx, ox], axis=1)\n",
    "    lstm_out  = tf.concat([im, fm, cm, om], axis=1)\n",
    "    lstm_bias = tf.concat([ib, fb, cb, ob], axis=1)\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #input_gate  = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        #update      = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        \n",
    "        # matmul(input) + matmul(output) + bias\n",
    "        gates = tf.matmul(i, lstm_in) + tf.matmul(o, lstm_out) + lstm_bias\n",
    "        input_gate, forget_gate, update, output_gate = tf.split(gates, 4, axis=1)\n",
    "        input_gate  = tf.sigmoid(input_gate)\n",
    "        forget_gate = tf.sigmoid(forget_gate)\n",
    "        ouput_gate  = tf.sigmoid(output_gate)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        5.0, global_step, 4000, 0.25, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "     # Optimizer.\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "   \n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295843 learning rate: 5.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "nmzyazgosivsqtzhdyjqsjhjowsirn  vxuafshtidwdbmlfnjty qqkff qno ioeaeqo desejbcsr\n",
      "ecmgwa iosnz nqeahelftjrr ppbgfg deitid qmzwzepojaprg fxoarxikdbtklffwbfodndjazz\n",
      "ulhkycwhgfkqea bndaqwfldjszoodhmsmoavfgofqrktuluuqrlowvfvhgpntrhswkx yxyoqsnxd i\n",
      "omyrtnpmnppudwpz rgkqntiyxlgzo wxfuiptbxykukvgyuo mumowunljax e f  vpjptokhtk  t\n",
      "enmc rmt b kcxxklamrudck cupav oirixil siuuu ey nfxn rnjmtpqrdihta sqwjxa naiq x\n",
      "================================================================================\n",
      "Validation set perplexity: 22.87\n",
      "Average loss at step 100: 2.870388 learning rate: 5.000000\n",
      "Minibatch perplexity: 17.81\n",
      "Validation set perplexity: 17.13\n",
      "Average loss at step 200: 2.733835 learning rate: 5.000000\n",
      "Minibatch perplexity: 13.26\n",
      "Validation set perplexity: 11.45\n",
      "Average loss at step 300: 2.420560 learning rate: 5.000000\n",
      "Minibatch perplexity: 10.72\n",
      "Validation set perplexity: 10.48\n",
      "Average loss at step 400: 2.318535 learning rate: 5.000000\n",
      "Minibatch perplexity: 9.97\n",
      "Validation set perplexity: 10.17\n",
      "Average loss at step 500: 2.261519 learning rate: 5.000000\n",
      "Minibatch perplexity: 9.66\n",
      "Validation set perplexity: 9.70\n",
      "Average loss at step 600: 2.173354 learning rate: 5.000000\n",
      "Minibatch perplexity: 8.32\n",
      "Validation set perplexity: 9.05\n",
      "Average loss at step 700: 2.129281 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 9.29\n",
      "Average loss at step 800: 2.110425 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 8.98\n",
      "Average loss at step 900: 2.070964 learning rate: 5.000000\n",
      "Minibatch perplexity: 8.36\n",
      "Validation set perplexity: 8.54\n",
      "Average loss at step 1000: 2.035554 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.10\n",
      "================================================================================\n",
      "kling loenoction one oncgian cand and the of six chusle of camen to the one five\n",
      "d hing the publoniols stoutay ative posticld ar the mamsmang bulr surle is hich \n",
      "fad coman the re worle hinials arsoalof muarts ht ept xevt boial co sided yspeng\n",
      "k buch wicz soltlagia pulivea to hnelly rabilo ebotions prjo k alp moess and eli\n",
      "moptaini fralrdent goce isto an to hiale of srogk wot ho hex of ceomuay mourle i\n",
      "================================================================================\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 1100: 2.017850 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 1200: 1.987593 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 1300: 1.937895 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 1400: 1.929557 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 1500: 1.909777 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.78\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 1600: 1.932445 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 1700: 1.896051 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 1800: 1.883218 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 1900: 1.825196 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 2000: 1.845805 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.67\n",
      "================================================================================\n",
      "k reatwomially mus he shanf theughory was s mato the to mecginkrophers commint a\n",
      "if main vant ibrdo fed ear lopabutional which the ia or hest consinuagre with la\n",
      "org uniblutipn ppe one nine nine nine eight and ray yearmooin soun hi its arso c\n",
      "jucse therecain marter acedgrexted beintur and is ansolud mu howening two father\n",
      " cossino nlent one combist of trign cotics one nine six sour aphies in the duchi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 2100: 1.820152 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 2200: 1.807881 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 2300: 1.793451 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 2400: 1.778542 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 2500: 1.780377 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 2600: 1.765663 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 2700: 1.754342 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 2800: 1.757925 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 2900: 1.727576 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 3000: 1.738615 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "onawemenn re blet pordilatith anneed in thhildbna of bo relations wren crpy zelo\n",
      "ke mustrekas called from yavout bry meligatical merbur to noy adt bicl in two tw\n",
      "anion am eiun allis are one nine one migral and an vednand rasent cher language \n",
      "vouphed for ading kit as and a mitween craed attroman in one nine five two six t\n",
      "ce place fish seven through ben suchaan more brother for gorn and reculice apacc\n",
      "================================================================================\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 3100: 1.715317 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 3200: 1.700932 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 3300: 1.715402 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 3400: 1.705012 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 3500: 1.669845 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 3600: 1.686527 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 3700: 1.700704 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 3800: 1.723046 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 3900: 1.689643 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 4000: 1.679295 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "dration ulsorrates sica fusic f series obgacticly of the with the partion landoy\n",
      " yessel of succuadhavial teld marcedok epsoloophy after bherf unomeon lassept on\n",
      "ning and gututar zersing cols to zol phois polutury ed the elf entert in the sot\n",
      "contern is two the loyiary of he drylini inte ockrents a cay quarter arran farin\n",
      "vected in factions hanctot emparome and arduer of norts anrips hust boging owreq\n",
      "================================================================================\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 4100: 1.664170 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 4200: 1.682640 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 4300: 1.648243 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4400: 1.641003 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4500: 1.634212 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4600: 1.600464 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4700: 1.630958 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4800: 1.634610 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4900: 1.624850 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 5000: 1.634905 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.62\n",
      "================================================================================\n",
      "forms x defendent been one throu catents community overning the he was the sains\n",
      "gand of bulations if or moctrications and has c recrusuity is of the claild coll\n",
      "xa ofserlar  era of g selker haw compattly is three severm of the great sub cite\n",
      "nant arair coded citcen life two three one seven nine eight nine one four nine s\n",
      "srient as two one nine eight one nine eight nine nine yee rivertherh isoniba ena\n",
      "================================================================================\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 5100: 1.634245 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5200: 1.616946 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 5300: 1.602750 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5400: 1.623174 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5500: 1.630884 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5600: 1.640841 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5700: 1.630221 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5800: 1.609471 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 5900: 1.601381 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 6000: 1.627194 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "ds in one permor uvawiok technry stable on u oge or rith batts president will na\n",
      "umy one nine seven a the ult me united stenting distablishimedys to the odich ea\n",
      "the not published storogs implie zero p the greegurer andond thess was dight and\n",
      "ololy noil suchers fines navioksedmed by a lint veracie language abpry is pernee\n",
      "ximish diglefers exuming shucks in lextinge wide force for town provectirm betwe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 6100: 1.616970 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 6200: 1.631442 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 6300: 1.629630 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 6400: 1.623675 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 6500: 1.615552 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 6600: 1.617972 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 6700: 1.617667 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 6800: 1.616237 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 6900: 1.642517 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 7000: 1.627810 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.94\n",
      "================================================================================\n",
      "usse bringualesut for intoled and and scary for extelms takesseinostagoos of tha\n",
      "nelia decond u ccaper herald other king reteaded as a abaid al vires jeen the ca\n",
      "ground a the some ecausely p find which facimessoulizal germedian this referenct\n",
      "xume varger and most exppecationally a ongiverss describer suncourted in folodom\n",
      "ugedonal held radical were which playal capan he loys vation digay facheled extr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 7100: 1.609160 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 7200: 1.621723 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 7300: 1.611086 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 7400: 1.620998 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 7500: 1.614596 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 7600: 1.590494 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 7700: 1.604506 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 7800: 1.627717 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 7900: 1.602134 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 8000: 1.600943 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "vena one of largradiands and hijm promany and day core in who k old that area th\n",
      "ge the yould heart actually and history petform of a wards they can befied go ko\n",
      "loghed is for methout of collectured include abold to appough the velods arein n\n",
      "changed mosts to fan annyspect form campaines incmeh coubn into all and last or \n",
      "riblination evety lish tpripilized of finale three society age preventry featy t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 8100: 1.615538 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 8200: 1.628185 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 8300: 1.573329 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 8400: 1.589352 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 8500: 1.593731 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 8600: 1.606297 learning rate: 0.312500\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 8700: 1.583199 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 8800: 1.577577 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 8900: 1.554128 learning rate: 0.312500\n",
      "Minibatch perplexity: 3.91\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 9000: 1.558713 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "zos tho to revisions popula court in the speqience live tai fromate but termon w\n",
      "culted arist illas premitic languages used to same of the lablicy and an these f\n",
      "realize considerogeson also proposed this doused aftinler standing the formated \n",
      "bias of gweeled however which place english apportted of linior over filmest is \n",
      "x tow plasis cily to marry is one nine six john the massion of south have deceni\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.55\n",
      "Average loss at step 9100: 1.565721 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 9200: 1.564931 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 9300: 1.550984 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 9400: 1.527690 learning rate: 0.312500\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 9500: 1.561438 learning rate: 0.312500\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 9600: 1.562469 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 9700: 1.531055 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 9800: 1.536213 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 9900: 1.557009 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 10000: 1.555126 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.03\n",
      "================================================================================\n",
      "one may eastronship feetter was a mety of a clspies used the por servize of the \n",
      "zingnaqusl worldy locks of expeniumed to sofr sudie shindathoob jaths on the ade\n",
      "vouative the govompions resistital morea kmopers using into a was nale the engyg\n",
      " carrinch of is one adimborial spoor as the to samed appeird their the tex fearh\n",
      "nessic called arder sear the bellochcle and rongitaclawaust were all protres and\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 10100: 1.585476 learning rate: 0.312500\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 10200: 1.565270 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 10300: 1.553343 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 10400: 1.560727 learning rate: 0.312500\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 10500: 1.568306 learning rate: 0.312500\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 10600: 1.584163 learning rate: 0.312500\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 10700: 1.559029 learning rate: 0.312500\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 10800: 1.565526 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 10900: 1.601142 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 11000: 1.576499 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "chonesise is include he de counces exphomers could but was order thm postant hal\n",
      "val and which that computation specuence for but preservers and used in the witt\n",
      "frendary beruiso noriely not crematter embt submrominist in obwalling and maperi\n",
      " centingage the beciuss expenta in the east fital mounter the untwrity cavious n\n",
      "jyders majories change cities has may it is shaws of isurcated hearthogcs arour \n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 11100: 1.591520 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 11200: 1.596007 learning rate: 0.312500\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 11300: 1.590811 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 11400: 1.552582 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 11500: 1.533087 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 11600: 1.545534 learning rate: 0.312500\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 11700: 1.559648 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 11800: 1.578566 learning rate: 0.312500\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 11900: 1.577642 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 12000: 1.571647 learning rate: 0.078125\n",
      "Minibatch perplexity: 3.98\n",
      "================================================================================\n",
      "lop and workis and inclusion and bill in verstable sporchem with the tent is as \n",
      "est rale serie papased since the induentually howoe transily brmekann s rul luss\n",
      "jon uband bu revelding grain emperbal toon an nown of the genor trom called unde\n",
      "n and engine in knew model ig right of s present loalinge and all setathile in s\n",
      "uug worked the th ply problemon and dues two go missuences the kutablantion vers\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 12100: 1.577669 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 12200: 1.613428 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 12300: 1.581302 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 12400: 1.604304 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 12500: 1.592451 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 12600: 1.588912 learning rate: 0.078125\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 12700: 1.586132 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 12800: 1.561807 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 12900: 1.572421 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 13000: 1.559689 learning rate: 0.078125\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      " imagings the they machine producer and gunion her american ptaked hues hon and \n",
      "ke wire immersames one rays tacks neqtenspachic north demital to bockside romina\n",
      "xgeorarily achifered as nibed rider governed forcest contrivation unciped provic\n",
      "woden canof very peamer one nine th the b citout grack the is novessite which ca\n",
      "sims largkmic call in letam to air their nations was have swekmbcol eastrasis ha\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 13100: 1.568658 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 13200: 1.531342 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 13300: 1.550834 learning rate: 0.078125\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 13400: 1.532029 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 13500: 1.535131 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 13600: 1.549074 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13700: 1.556996 learning rate: 0.078125\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13800: 1.572778 learning rate: 0.078125\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 13900: 1.525905 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 14000: 1.522004 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      "formation of cath ryplate one cendo one nine ownly dolbrellage bullegelle nine e\n",
      "us synu in has entricteian hi zero two sex one zero zero nine zero four eight se\n",
      "ciating oni kord war confaco from defense the rirbulars methogious sema congress\n",
      "y every all sjust of because to has music fiences that the probut blight gynumbe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w rangery bo hoave hend or ember one milding becaume is abell where fillion s ra\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 14100: 1.537746 learning rate: 0.078125\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 14200: 1.576647 learning rate: 0.078125\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14300: 1.562415 learning rate: 0.078125\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14400: 1.583480 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 14500: 1.552562 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 14600: 1.554564 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 14700: 1.572079 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 14800: 1.553048 learning rate: 0.078125\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 14900: 1.573346 learning rate: 0.078125\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 15000: 1.588770 learning rate: 0.078125\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "ni leftler nigin majordrsliples or one five darondarla there actoritary huring t\n",
      "ding speciously user loandate which midels a radaine use ileed by memptea of the\n",
      "ge surged by syough and stroving locrature it is life rave fictermince capities \n",
      "ith must one nine seven one eight bcypin exist to arcommunication and the august\n",
      "varce an armertlix military two zero zero conventing appear settre his is confir\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "summary_frequency = 100\n",
    "run_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
